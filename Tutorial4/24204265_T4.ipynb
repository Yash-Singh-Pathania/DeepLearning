{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name   : Yash Singh Pathania \n",
    "# Student Number : 24204265\n",
    "# Note : Print Statements Are for debugging/understanding can be ignored \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Neural Network #\n",
    "\n",
    "Objectives: built and train a deep layer feedforward neural network building on the single feedfoward neural network implementation of the previous session notebook.\n",
    "\n",
    "> **Instructions:** ensure your Python environment is setup with the following additional packages: \n",
    ">\n",
    "> - `t4utils.py` contains unit tests to check your code\n",
    "\n",
    "**Notation:** superscript $[k]$ refers to quantities associated with the $k$-th layer.\n",
    "\n",
    "<img src=\"figs/deepnetwork.png\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import t4utils as t4\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of steps to implement a deep neural network model:\n",
    "\n",
    "1. Initialize the parameters of a $K$-layer neural network.\n",
    "2. Implement the forward propagation:\n",
    "    - compute linear component $z^{[k]}=a^{[k-1]} W^{[k]} + b^{[k]}$\n",
    "    - compute activation $a^{[k]} = g(z^{[k]})$, where $g(\\cdot)$ is the layer non-linearity (activation function). We'll use both ReLU and sigmoid.\n",
    "    - combine $K-1$ ReLU layers with a final sigmoid layer ($K$ layer in total)\n",
    "    - compute all layer outputs\n",
    "    - evaluate loss from final layer activation\n",
    "3. Implement the backward propagation:\n",
    "    - compute the gradients for the linear component and the non-linear activation function\n",
    "    - perform backprop on sigmoid layer followed by $K-1$ ReLU layers\n",
    "    - evaluate gradients for all parameters $W^{[k]}, b^{[k]}$\n",
    "4. Update parameters using learning rate\n",
    "\n",
    "To implement our learning algorithm, we will associate a backprop module for every forward function. At every step of your \n",
    "forward evaluation, we will store results in a cache required to compute gradients during backprop. As a result, training a network using backprop requires about twice more memory than inference. With deep learning frameworks, your generally describe your network as graph and differentiation is performed automatically for training model parameters.\n",
    "\n",
    "## A. Load Dataset and Initialise Model ##\n",
    "\n",
    "First, let's generate a dataset to work on. We will use the `catsvsnoncats` image dataset used in the logistic regression notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath='datasets/catsvsnoncats.h5'):\n",
    "    \"\"\"\n",
    "    Load and pre-process dataset\n",
    "    \n",
    "    Arguments:\n",
    "    filepath -- string, dataset path\n",
    "\n",
    "    Returns:\n",
    "    (X_train, Y_train), (X_test, Y_test), classes -- training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    h5file = h5py.File(filepath, \"r\")\n",
    "    X_train = np.array(h5file[\"X_train\"][:])\n",
    "    Y_train = np.array(h5file[\"Y_train\"][:])\n",
    "    X_test = np.array(h5file[\"X_test\"][:])\n",
    "    Y_test = np.array(h5file[\"Y_test\"][:])\n",
    "    classes = np.array(h5file[\"Classes\"][:]) \n",
    "    h5file.close()\n",
    "\n",
    "    # Reshape and scale datasets containing N_train and N_test mages such that\n",
    "    # X_train.shape = (n_train, W * H * 3)\n",
    "    # Y_train.shape = (n_train, 1)\n",
    "    # X_test.shape = (n_test, W * H * 3)\n",
    "    # X_test.shape = (n_test, 1)\n",
    "    # Scale X_train and X_test from range {0..255} to (0,1)\n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255\n",
    "    X_test  = X_test.reshape(X_test.shape[0], -1) / 255\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "    Y_test  = Y_test.reshape(Y_test.shape[0], 1)\n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "    print (' After X_trian') # Just for debugging \n",
    "    return (X_train, Y_train), (X_test, Y_test), classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now configure our model topology i.e. K-1 ReLU activated layers and one final sigmoid activated layer. Initialise bias to zeros and weights from $\\mathcal{N}(0,\\sigma)$ with $\\sigma = 1 /\\sqrt{n_h^{[k]}}$ where $n_h^{[k]}$ is number of units in the $k$-th layer.\n",
    "\n",
    "The number of units of the first layer is determined by the number of features of our dataset, $x$. Similarly the number of units of the final output layer is determined by the dimension of $y$. The number of units for the remaining $K-1$ hidden layers is specified as an array of integers.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Setup the model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(X, Y, n_h=[128, 64, 16, 8]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- ground truth label, column vector of shape (n, n_y)\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    n_x = X.shape[1] # size of input layer\n",
    "    n_y = Y.shape[1] # size of output layer\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    dims = sum([[n_x], n_h, [n_y]], [])\n",
    "    K = len(dims) # number of network layers\n",
    "\n",
    "    params = {}\n",
    "    \n",
    "    for k in range(1, K):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = np.random.randn(dims[k - 1], dims[k]) * 1/np.sqrt(dims[k-1])  # Initialize weights with small random values\n",
    "        params['b{}'.format(k)] = np.zeros((1, dims[k]))  # Initialize biases to zeros\n",
    "        ### END OF YOUR CODE SEGMENT ###                                            \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    assert(X.shape[0] == (Y.shape[0]))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.9195  -0.21968  0.58   ]\n",
      " [-0.28855 -0.51552 -0.41944]\n",
      " [ 0.15847  0.17663  0.75786]\n",
      " [-0.18893 -0.54634 -0.32417]]\n",
      "b1 = [[0. 0. 0.]]\n",
      "W2 = [[ 0.85447 -1.2155 ]\n",
      " [ 0.13204  0.20821]\n",
      " [-0.83132 -0.38261]]\n",
      "b2 = [[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X, Y, n_h = t4.model_config_test()\n",
    "params = model_config(X, Y, [3])\n",
    "for k in range(1, len(params) // 2 + 1):\n",
    "    print(\"W{} = {}\".format(k, params['W{}'.format(k)]))\n",
    "    print(\"b{} = {}\".format(k, params['b{}'.format(k)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use np.random with multiplication wiht 0.1 to keep them small for initial input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> W1 = [[ 0.9195  -0.21968  0.58   ]\n",
    ">       [-0.28855 -0.51552 -0.41944]\n",
    ">       [ 0.15847  0.17663  0.75786]\n",
    ">       [-0.18893 -0.54634 -0.32417]]\n",
    "> b1 = [[0. 0. 0.]]\n",
    "> W2 = [[ 0.85447 -1.2155 ]\n",
    ">       [ 0.13204  0.20821]\n",
    ">       [-0.83132 -0.38261]]\n",
    "> b2 = [[0. 0.]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "## B. Forward Propagation ##\n",
    "\n",
    "We will now implement the forward propagation for the linear component of the $k$-th layer $z_k = a^{[k-1]} W^{[k]} + b^{[k]}$ and its activation $a^{[k]} = g(z^{[k]})$, where $g(\\cdot)$ is either the ReLU or the sigmoid function. Note that $a^{[0]} = x$. We should also return a cache for the backward propagation.\n",
    "\n",
    "For $n$ samples stacked in the rows of a input matrix $X$, the linear and activation parts are given by $Z_k = A^{[k-1]} W^{[k]} + b^{[k]}$ and $A^{[k]} = g(Z^{[k]})$\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Implement forward propagation for the linearity and both activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for linearity\n",
    "def linear_fwd(W, b, A):\n",
    "    \"\"\"\n",
    "    Linearity\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A -- input, shape (n, n_hk-1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- linear output, shape (n, n_hk)\n",
    "    cache -- dictionary for back propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    Z = np.dot(A, W) + b\n",
    "    cache = {\"W\": W, \"b\": b, \"A_prev\": A}\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]))\n",
    "    return Z, cache\n",
    "\n",
    "# Forward propagation for sigmoid non-linearity\n",
    "def sigmoid_fwd(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A = 1 / (1 + np.exp(-Z))  # Calculate sigmoid activation\n",
    "    cache = {\"Z\": Z}          # Store Z for backpropagation\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n",
    "\n",
    "# Forward propagation for ReLU non-linearity\n",
    "def relu_fwd(Z):\n",
    "    \"\"\"\n",
    "    RELU activation\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of ReLU(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A = np.maximum(Z, 0)      # Calculate ReLU activation \n",
    "    cache = {\"Z\": Z}          # Store Z for backpropagation\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z.T = [[0.71591 1.45199]]\n",
      "A.T = ReLU(Z.T) = [[0.71591 1.45199]]\n",
      "A.T = sigmoid(Z.T) = [[0.67171 0.8103 ]]\n"
     ]
    }
   ],
   "source": [
    "W, b, A = t4.linear_fwd_test()\n",
    "Z, cache = linear_fwd(W, b, A)\n",
    "print(\"Z.T = {}\".format(Z.T))\n",
    "A, cache = relu_fwd(Z)\n",
    "print(\"A.T = ReLU(Z.T) = {}\".format(A.T))\n",
    "A, cache = sigmoid_fwd(Z)\n",
    "print(\"A.T = sigmoid(Z.T) = {}\".format(A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> Z.T = [[0.71591 1.45199]]\n",
    "> A.T = ReLU(Z.T) = [[0.71591 1.45199]]\n",
    "> A.T = sigmoid(Z.T) = [[0.67171 0.8103 ]]\n",
    "> ```\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Implement the single layer forward propagation<br>\n",
    "> Combine the forward modules to compute a single layer forward propagation $A^{[k]} = g(Z^{[k]}) = g(A^{[k-1]} W^{[k]})$, where $g(\\cdot)$ is the non-linearity (either ReLU or Sigmoid). Also return the cache for the layer's backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer forward propagation for \n",
    "def singlelayer_fwd(W, b, A_prev, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer forward propagation (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A_prev -- input, shape (n, n_hk-1)\n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of g(A_prev @ W + b), shape (n, n_hk)\n",
    "    cache -- dictionary for backprop\n",
    "        LINEAR -- dictionary linear cache\n",
    "        ACTIVATION -- dictionary activation cache        \n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    Z, linear_cache = linear_fwd(W, b, A_prev)\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    if non_linearity == 'ReLU':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        A, activation_cache = relu_fwd(Z)\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    elif non_linearity == 'Sigmoid':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        A, activation_cache = sigmoid_fwd(Z)\n",
    "        ### END OF YOUR CODE SEGMENT ###\n",
    "    \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    return A, {'LINEAR': linear_cache, 'ACTIVATION': activation_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_Sigmoid.T = [[0.67171 0.8103 ]]\n",
      "cache = {'LINEAR': {'W': array([[ 0.57376],\n",
      "       [ 0.28773],\n",
      "       [-0.23563]]), 'b': array([[0.95349]]), 'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
      "       [ 1.33186, -0.36187,  0.68561]])}, 'ACTIVATION': {'Z': array([[0.71591],\n",
      "       [1.45199]])}}\n",
      "A_ReLU.T = [[0.71591 1.45199]]\n",
      "cache = {'LINEAR': {'W': array([[ 0.57376],\n",
      "       [ 0.28773],\n",
      "       [-0.23563]]), 'b': array([[0.95349]]), 'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
      "       [ 1.33186, -0.36187,  0.68561]])}, 'ACTIVATION': {'Z': array([[0.71591],\n",
      "       [1.45199]])}}\n"
     ]
    }
   ],
   "source": [
    "W, b, A_prev = t4.singlelayer_fwd_test()\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'Sigmoid')\n",
    "print(\"A_Sigmoid.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'ReLU')\n",
    "print(\"A_ReLU.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> A_Sigmoid.T = [[0.67171 0.8103 ]]\n",
    "> cache = {'LINEAR': {'W': array([[ 0.57376],\n",
    ">                                 [ 0.28773],\n",
    ">                                 [-0.23563]]), \n",
    ">                     'b': array([[0.95349]]), \n",
    ">                     'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
    ">                                      [ 1.33186, -0.36187,  0.68561]])}, \n",
    ">                     'ACTIVATION': {'Z': array([[0.71591],\n",
    ">                                                [1.45199]])}}\n",
    "> A_ReLU.T = [[0.71591 1.45199]]\n",
    "> cache = {'LINEAR': {'W': array([[ 0.57376],\n",
    ">                                 [ 0.28773],\n",
    ">                                 [-0.23563]]), \n",
    ">                     'b': array([[0.95349]]), \n",
    ">                     'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
    ">                                      [ 1.33186, -0.36187,  0.68561]])},\n",
    ">                     'ACTIVATION': {'Z': array([[0.71591],\n",
    ">                                               [1.45199]])}}\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Create a $K$-layer deep model by stacking single layer forward propagation. The first $K-1$ layers use the ReLU non-linearity for their activation function and the sigmoid function is applied on the final layer.\n",
    "\n",
    "The loss for $n_{train}$ stacked examples is given by:\n",
    "$$\\mathcal{L}(Y, A^{[K]}) = -\\frac{1}{n_{train}}\\left(\n",
    "Y^T\\, \\log A^{[K]} + (1-Y)^T\\,\\log(1-A^{[K]})\n",
    "\\right)$$\n",
    "\n",
    "where $A^{[K]}$ is the final layer activation.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Implement the $K$-layer forward propagation and compute the loss<br>\n",
    ">\n",
    "> **Note** that it is not possible to vectorise the network depth and a loop must be used. This is the only loop in our model. Keep a record of the caches for all $K$ layers in an python list (use `list.append(element)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation (inference)\n",
    "def forward_prop(params, X, Y=None):\n",
    "    \"\"\"\n",
    "    Compute the layer activations and loss if needed\n",
    "\n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- optional argument, ground truth label, column vector of shape (n, n_y)\n",
    "\n",
    "    Returns:\n",
    "    A -- final layer output (activation value) \n",
    "    loss -- cross-entropy loss or NaN if Y=None\n",
    "    caches -- array of caches for the K layers\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    K = len(params) // 2  # Integer division to get the number of layers\n",
    "    A = X\n",
    "    \n",
    "    # K-1 [Linear->ReLU] layer\n",
    "    for k in range(1, K):\n",
    "        A_prev = A\n",
    "        ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "        W = params['W{}'.format(k)]\n",
    "        b = params['b{}'.format(k)]\n",
    "        A, cache = singlelayer_fwd(W, b, A_prev, non_linearity='ReLU')\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "        caches.append(cache)\n",
    "\n",
    "    # 1 [Linear->Sigmoid] layer\n",
    "    A_prev = A\n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    W = params['W{}'.format(K)]  # Access the last layer's weights\n",
    "    b = params['b{}'.format(K)]  # Access the last layer's bias\n",
    "    A, cache = singlelayer_fwd(W, b, A_prev, non_linearity='Sigmoid') # Sigmoid for output\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    caches.append(cache)\n",
    "    \n",
    "    loss = float('nan')\n",
    "    if Y is not None:\n",
    "        Y_hat = A\n",
    "        n = Y.shape[0]\n",
    "        # Compute the cross-entropy loss\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        loss = (-1/n) * np.sum(Y*np.log(Y_hat) + (1-Y)*np.log(1-Y_hat))\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "        loss = np.squeeze(loss)  # Makes sure loss is a scalar.\n",
    "        assert(loss.dtype == float)\n",
    "        \n",
    "    assert(A.shape == (X.shape[0], W.shape[1]))\n",
    "    return A, loss, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.T = [[0.10101 0.33321 0.01484 0.00042]]\n",
      "loss = 0.67831\n",
      "3 caches\n"
     ]
    }
   ],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "A, loss, caches = forward_prop(params, X, Y)\n",
    "print(\"A.T = {}\".format(A.T))\n",
    "print(\"loss = {:.5f}\".format(loss))\n",
    "print(\"{} caches\".format(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***<br>\n",
    ">\n",
    "> ```\n",
    "> A.T = [[0.10101 0.33321 0.01484 0.00042]]\n",
    "> loss = 0.67831\n",
    "> 3 caches\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "## C. Backward Propagation ##\n",
    "\n",
    "Backward propagation is implemented similarly to the forward propagation, module by module using the forward prop cache to compute gradients of the loss with respect to the model parameters $\\theta \\equiv (W^{[1]}, b^{[1]}, ..., W^{[K]}, b^{[K]})$.\n",
    "\n",
    "<img src=\"figs/backprop.png\" style=\"width:600px;\">\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Implement backward propagation for the linearity and both activation functions. <br>\n",
    ">\n",
    "> Use the following gradient expressions for the linear backprop:\n",
    "> \n",
    "> $$dW^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial W^{[k]}} = \\frac{1}{n}  A^{[k-1]T}\\,dZ^{[k]} \\notag$$\n",
    ">\n",
    "> $$db^{[k]}_j\\equiv \\frac{\\partial \\mathcal{L} }{\\partial b_j^{[k]}} =\\frac{1}{n} \\sum_{i=1}^{n} dZ^{[2]}_{i\\,j} \\notag$$\n",
    ">\n",
    "> $$dA^{[k-1]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial A^{[k-1]}} = dZ^{[k]} W^{[k] T}  \\notag$$\n",
    ">\n",
    "> if $g(\\cdot)$ denotes the activation function (ReLU or Sigmoid), the gradient is as follows:\n",
    "> \n",
    "> $$dZ^{[k]} = dA^{[k]} \\odot g'(Z^{[k]}) \\notag$$.  \n",
    "> \n",
    "> Applying the above expression to the ReLU function yields the following backprop:\n",
    ">\n",
    "> $$dZ^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial Z^{[k]}} = dA^{[k]}\\odot\\mathbb 1\\text{{$Z^{[k]}$ $\\geqslant 0$}}\\notag$$\n",
    ">\n",
    "> For the Sigmoid backprop, the backprop expression is:\n",
    ">\n",
    "> $$dZ^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial Z^{[k]}} = dA^{[k]}\\odot\\sigma(Z^{[k]})\\odot\\left(1-\\sigma(Z^{[k]})\\right)\\notag$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation for linearity\n",
    "def linear_back(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linearity backprop\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of loss with respect to current layer linear output\n",
    "    cache -- dictionary from forward propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- previous layer activation input\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (7 lines)\n",
    "    # Use dictionary keys instead of tuple unpacking\n",
    "    W = cache[\"W\"]\n",
    "    b = cache[\"b\"]\n",
    "    A_prev = cache[\"A_prev\"]\n",
    "    n = A_prev.shape[0]\n",
    "    dW = np.dot(A_prev.T, dZ) / n\n",
    "    db = np.sum(dZ, axis=0, keepdims=True) / n\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dW, db, dA_prev\n",
    "\n",
    "# Backward propagation for ReLU non-linearity\n",
    "def relu_back(dA, cache):\n",
    "    \"\"\"\n",
    "    ReLU backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    # Extract Z from dictionary\n",
    "    Z = cache[\"Z\"]\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "# Backward propagation for sigmoid non-linearity\n",
    "def sigmoid_back(dA, cache):\n",
    "    \"\"\"\n",
    "    Sigmoid backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    # Extract Z from dictionary\n",
    "    Z = cache[\"Z\"]\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * S * (1 - S)\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW.T = [[0.12038 0.0907  0.15756]]\n",
      "db = [[0.30189]]\n",
      "dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
      " [-0.19356  0.78325 -1.38795]]\n",
      "ReLU: dZ = [[-0.21768  0.82146  0.     ]\n",
      " [ 1.33186  0.       0.     ]]\n",
      "Sigmoid: dZ = [[-0.05018  0.20117  0.36523]\n",
      " [ 0.26743 -0.0476   0.1664 ]]\n"
     ]
    }
   ],
   "source": [
    "dZ, cache = t4.linear_back_test()\n",
    "dW, db, dA_prev = linear_back(dZ, cache)\n",
    "print(\"dW.T = {}\".format(dW.T))\n",
    "print(\"db = {}\".format(db))\n",
    "print(\"dA_prev = {}\".format(dA_prev))\n",
    "A, cache = t4.non_linearity_test()\n",
    "dZ = relu_back(A, cache)\n",
    "print(\"ReLU: dZ = {}\".format(dZ))\n",
    "dZ = sigmoid_back(A, cache)\n",
    "print(\"Sigmoid: dZ = {}\".format(dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> dW.T = [[0.12038 0.0907  0.15756]]\n",
    "> db = [[0.30189]]\n",
    "> dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
    ">            [-0.19356  0.78325 -1.38795]]\n",
    "> ReLU: dZ = [[-0.21768  0.82146  0.     ]\n",
    ">             [ 1.33186  0.       0.     ]]\n",
    "> Sigmoid: dZ = [[-0.05018  0.20117  0.36523]\n",
    ">                [ 0.26743 -0.0476   0.1664 ]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "We can now combine the backprop from the linear and non-lineary modules to compute the backprop for a single activation layer.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Implement the single layer backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelayer_back(dA, cache, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer backprop (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        LINEAR -- dictionary from forward linear propagation \n",
    "        ACTIVATION -- dictionary from forward non-linearity propagation \n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache = cache['LINEAR']\n",
    "    activation_cache = cache['ACTIVATION']\n",
    "    \n",
    "    if non_linearity == 'Sigmoid':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        dZ = sigmoid_back(dA, activation_cache)\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "    elif non_linearity == 'ReLU':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        dZ = relu_back(dA, activation_cache)\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    dW, db, dA_prev = linear_back(dZ, linear_cache)\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: dW.T = [[-0.16122 -0.14496  0.03939]]\n",
      "ReLU: db = [[-0.10884]]\n",
      "ReLU: dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
      " [ 0.       0.       0.     ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU: db = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(db))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU: dA_prev = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dA_prev))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma\u001b[49m)\n\u001b[1;32m      8\u001b[0m dW, db, dA_prev \u001b[38;5;241m=\u001b[39m singlelayer_back(dA, cache, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigmoid: dW.T = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dW\u001b[38;5;241m.\u001b[39mT))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "dA, cache = t4.singlelayer_back_test()\n",
    "\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'ReLU')\n",
    "print(\"ReLU: dW.T = {}\".format(dW.T))\n",
    "print(\"ReLU: db = {}\".format(db))\n",
    "print(\"ReLU: dA_prev = {}\".format(dA_prev))\n",
    "print(a)\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'Sigmoid')\n",
    "print(\"Sigmoid: dW.T = {}\".format(dW.T))\n",
    "print(\"Sigmoid: db = {}\".format(db))\n",
    "print(\"Sigmoid: dA_prev = {}\".format(dA_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***<br>\n",
    ">\n",
    "> ```\n",
    "> ReLU: dW.T = [[-0.16122 -0.14496  0.03939]]\n",
    "> ReLU: db = [[-0.10884]]\n",
    "> ReLU: dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
    ">                  [ 0.       0.       0.     ]]\n",
    "> Sigmoid: dW.T = [[0.02563 0.01894 0.03751]]\n",
    "> Sigmoid: db = [[0.06896]]\n",
    "> Sigmoid: dA_prev = [[ 0.01282 -0.05188  0.09194]\n",
    ">                     [-0.04532  0.18338 -0.32496]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "Now that we have the backprop implemented for a single layer, we can compute the backprop for a $K$-layer network iteratively. At each iteration, we use the layer cache computed during forward propagation, initialising the back-propagation with $dA^{[K]}\\equiv \\frac{\\partial \\mathcal{L}}{\\partial A^{[K]}}$, given for sample $i$ as\n",
    "\n",
    "$$ dA_i^{[K]} \\equiv \\frac{\\partial \\mathcal{L}}{\\partial A_i^{[K]}} = - \\left( \\frac{Y_i}{A^{[K]}} - \\frac{1- Y_i}{1-A^{[K]}} \\right)$$\n",
    "\n",
    "Using the post-activation gradient $dA^{[K]}$, initialise the backprop chain and feed the backprop function for the last network layer (sigmoid). Going backward in the network with `for` loop, continue the backprop evaluation for the remaining $K-1$ layers (ReLU activation). Store each of the $dW^{[k]}, db^{[k]}, dA^{[k]}$ gradients, these will be required to update our model parameters.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Implement the model backprop<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward_propagation\n",
    "def back_prop(AK, Y, caches):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients\n",
    "    \n",
    "    Arguments:\n",
    "    AK -- probability vector, final layer output, shape (1, n_y)\n",
    "    Y -- ground truth output (n, n_y)\n",
    "    caches -- array of layer cache, len=K\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    n = AK.shape[0]\n",
    "    assert(Y.shape == AK.shape)\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    dAK = - (np.divide(Y, AK) - np.divide(1 - Y, 1 - AK))\n",
    "    cache = caches[-1]\n",
    "    cache[\"Z\"] = cache[\"ACTIVATION\"][\"Z\"] \n",
    "    grads[\"dW{}\".format(len(caches))], grads[\"db{}\".format(len(caches))],  grads[\"dA{}\".format(len(caches))] = singlelayer_back(dAK, cache, non_linearity='Sigmoid')\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    for k in reversed(range(K - 1)):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        cache = caches[k]\n",
    "        cache[\"Z\"] = cache[\"ACTIVATION\"][\"Z\"] \n",
    "        grads[\"dW{}\".format(k + 1)], grads[\"db{}\".format(k + 1)], grads[\"dA{}\".format(k + 1)] = singlelayer_back(grads[\"dA{}\".format(k + 2)], cache, non_linearity='ReLU')\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "        \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1.T = [[ 0.39632  0.35635 -0.09682  0.18344]\n",
      " [ 0.71849  0.64602 -0.17552  0.33255]\n",
      " [ 0.16057  0.14437 -0.03923  0.07432]]\n",
      "db1 = [[0.26756 0.48505 0.1084 ]]\n",
      "dA1 = [[-1.2351  -0.07942  2.06421 -1.18989]\n",
      " [ 0.       0.       0.       0.     ]]\n",
      "dW2.T = [[-0.68133  0.1135  -0.30305]]\n",
      "db2 = [[0.86059]]\n",
      "dA2 = [[0.53511 0.9701  0.2168 ]\n",
      " [0.47488 0.86091 0.1924 ]]\n"
     ]
    }
   ],
   "source": [
    "AK, Y, caches = t4.back_prop_test()\n",
    "grads = back_prop(AK, Y, caches)\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dA1 = {}\".format(grads['dA1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dA2 = {}\".format(grads['dA2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ``` \n",
    "> dW1.T = [[ 0.39632  0.35635 -0.09682  0.18344]\n",
    ">          [ 0.71849  0.64602 -0.17552  0.33255]\n",
    ">          [ 0.16057  0.14437 -0.03923  0.07432]]\n",
    "> db1 = [[0.26756 0.48505 0.1084 ]]\n",
    "> dA1 = [[-1.2351  -0.07942  2.06421 -1.18989]\n",
    ">        [ 0.       0.       0.       0.     ]]\n",
    "> dW2.T = [[-0.68133  0.1135  -0.30305]]\n",
    "> db2 = [[0.86059]]\n",
    "> dA2 = [[0.53511 0.9701  0.2168 ]\n",
    ">        [0.47488 0.86091 0.1924 ]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "The update rule for gradient descent is given by $\\theta = \\theta - \\lambda\\, d\\theta$, where $\\lambda$ denotes the learning rate. The choice for the value of hyper-parameter $\\lambda$ is important, a small value will lead to poor convergence while a value too large may result in divergence.\n",
    "\n",
    "In our deep network, model parameters $\\theta$ decribe the tuple\n",
    "$(W^{[1]}, b^{[1]}, \\ldots, W^{[K]}, b^{[K]})$.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Implement the update rule in function `update_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters\n",
    "def update_params(params, grads, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = params['W{}'.format(k)] - learning_rate * grads['dW{}'.format(k)]\n",
    "        params['b{}'.format(k)] = params['b{}'.format(k)] - learning_rate * grads['db{}'.format(k)]\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.T = [[-0.39654  1.51821  0.58204  1.00121]\n",
      " [ 0.7778  -0.33413  0.35043 -1.55824]\n",
      " [ 1.47163  0.72108 -0.23125 -0.4334 ]]\n",
      "b1 = [[-0.07123 -0.68594  0.23951]]\n",
      "W2.T = [[-0.14884  2.72671  0.61945]]\n",
      "b2 = [[0.74769]]\n"
     ]
    }
   ],
   "source": [
    "params, grads = t4.update_params_test()\n",
    "params = update_params(params, grads, 0.1)\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> W1.T = [[-0.39654  1.51821  0.58204  1.00121]\n",
    ">         [ 0.7778  -0.33413  0.35043 -1.55824]\n",
    ">         [ 1.47163  0.72108 -0.23125 -0.4334 ]]\n",
    "> b1 = [[-0.07123 -0.68594  0.23951]]\n",
    "> W2.T = [[-0.14884  2.72671  0.61945]]\n",
    "> b2 = [[0.74769]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "You can implement the training algorithm using gradient descent with backprop.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 9:**</font> Complete the function below to train your deep neural network model by repeatedly performing inference (forward propagation), evaluating parameter gradients using backprop, and updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(params, X, Y, epochs=2500, learning_rate=0.0075, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- groud truth label vector of size (n, n_y)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with optimised parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "        A, loss, caches = forward_prop(params, X, Y)\n",
    "        grads = back_prop(A, Y, caches)\n",
    "        params = update_params(params, grads, learning_rate)\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = [0.67831 0.39311 0.38196]\n",
      "W1.T = [[-0.93729  0.72484 -0.71946  0.90889 -0.95217]\n",
      " [ 0.06497  0.47414 -0.83891  0.21742  0.58681]\n",
      " [-3.22537  0.40678 -0.81217 -1.18205  1.06676]\n",
      " [ 1.02805  0.46416 -0.46209 -0.91687  0.2003 ]]\n",
      "b1 = [[-0.89367 -0.9791  -1.53733 -1.24609]]\n",
      "W2.T = [[ 0.33411 -0.79252  1.2068  -0.83628]\n",
      " [-0.17309  1.839   -0.57709  0.31693]\n",
      " [ 1.46725 -0.43936 -1.03104  0.35325]]\n",
      "b2 = [[ 1.44688 -0.37785 -1.12293]]\n",
      "W3.T = [[-0.48626  1.47999 -2.10593]]\n",
      "b3 = [[0.35461]]\n",
      "dW1.T = [[-0.00422  0.00449  0.03353  0.02765 -0.00591]\n",
      " [ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.01105  0.00226 -0.00011  0.00337 -0.0016 ]\n",
      " [ 0.       0.       0.       0.       0.     ]]\n",
      "db1 = [[ 0.01863  0.      -0.00654  0.     ]]\n",
      "dW2.T = [[ 0.02712  0.      -0.02508  0.     ]\n",
      " [ 0.       0.       0.       0.     ]\n",
      " [-0.00808  0.       0.       0.     ]]\n",
      "db2 = [[ 0.01661  0.      -0.00378]]\n",
      "dW3.T = [[-0.00584  0.       0.00361]]\n",
      "db3 = [[-0.03416]]\n"
     ]
    }
   ],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "params, grads, loss_log = model_fit(params, X, Y, epochs = 300, verbose=False)\n",
    "print(\"loss = {}\".format(np.array(loss_log)))\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))\n",
    "print(\"W3.T = {}\".format(params['W3'].T))\n",
    "print(\"b3 = {}\".format(params['b3']))\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dW3.T = {}\".format(grads['dW3'].T))\n",
    "print(\"db3 = {}\".format(grads['db3']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> loss = [0.67831 0.39311 0.38196]\n",
    "> W1.T = [[-0.93729  0.72484 -0.71946  0.90889 -0.95217]\n",
    ">         [ 0.06497  0.47414 -0.83891  0.21742  0.58681]\n",
    ">         [-3.22537  0.40678 -0.81217 -1.18205  1.06676]\n",
    ">         [ 1.02805  0.46416 -0.46209 -0.91687  0.2003 ]]\n",
    "> b1 = [[-0.89367 -0.9791  -1.53733 -1.24609]]\n",
    "> W2.T = [[ 0.33411 -0.79252  1.2068  -0.83628]\n",
    ">         [-0.17309  1.839   -0.57709  0.31693]\n",
    ">         [ 1.46725 -0.43936 -1.03104  0.35325]]\n",
    "> b2 = [[ 1.44688 -0.37785 -1.12293]]\n",
    "> W3.T = [[-0.48626  1.47999 -2.10593]]\n",
    "> b3 = [[0.35461]]\n",
    "> dW1.T = [[-0.00422  0.00449  0.03353  0.02765 -0.00591]\n",
    ">          [ 0.       0.       0.       0.       0.     ]\n",
    ">          [ 0.01105  0.00226 -0.00011  0.00337 -0.0016 ]\n",
    ">          [ 0.       0.       0.       0.       0.     ]]\n",
    "> db1 = [[ 0.01863  0.      -0.00654  0.     ]]\n",
    "> dW2.T = [[ 0.02712  0.      -0.02508  0.     ]\n",
    ">        [ 0.       0.       0.       0.     ]\n",
    ">        [-0.00808  0.       0.       0.     ]]\n",
    "> db2 = [[ 0.01661  0.      -0.00378]]\n",
    "> dW3.T = [[-0.00584  0.       0.00361]]\n",
    "> db3 = [[-0.03416]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Once the model parameters are optimised, we can use the deep feedforward neural network model to predict the class of a given input $x$. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 10:**</font> Implement the `model_predict()` function. <br/> <br/>\n",
    "> Perform model inference on input $X$ and convert the activation output to predictions (0/1 values): <br/>\n",
    ">\n",
    "> $\\hat{Y_i}={\\mathbb 1}_{A_i^{[K]}>1/2} = \n",
    "    \\begin{cases}\n",
    "      1 & \\text{if } A_i^{[K]}> 1/2 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    ">    \n",
    "> Avoid python iteration and `if` statements preferring numpy vectorial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference\n",
    "def model_predict(params, X):\n",
    "    \"\"\"\n",
    "    Predict class label using model parameters\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape (n, n_x)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- vector with class predictions for examples in X, shape (n, n_y)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    AK, _, _ = forward_prop(params, X)\n",
    "    Y_hat = (AK > 0.5).astype(int)\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    n = X.shape[0]\n",
    "    assert(Y_hat.shape == (n, 1))    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.T = [[0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X, _, params = t4.forward_prop_test()\n",
    "params['b1'] = np.zeros((1,4))\n",
    "Y_hat = model_predict(params, X)\n",
    "print(\"predictions.T = {}\".format(Y_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `predictions.T = [[0. 1. 0. 0.]]`\n",
    "\n",
    "***\n",
    "\n",
    "## D. Stitching it all together ##\n",
    "\n",
    "We can now combine all the previous step into a model function that will take our training and testing datasets as input along with our hyper-parameters (learning rate, number of epochs for training and number of units in hidden the layers). It will return trained model parameters along with training and testing logs useful to study the training behaviour of our backprop optimisation. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 11:**</font> Implement the following function and evaluate your model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep model\n",
    "def deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 7, 3], epochs=2500, learning_rate=0.007, verbose=True):\n",
    "    '''\n",
    "    Build, train and evalaute the K-layer model\n",
    "    (K-1) * [LINEAR -> RELU]  -> [LINEAR -> SIGMOID] \n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set a numpy array of shape (n_train, n_x)\n",
    "    Y_train -- training groud truth vector of size (n_train, n_y)\n",
    "    X_test -- testing set a numpy array of shape (n_test, n_x)\n",
    "    Y_test -- testing groud truth vector of size (n_test, n_y)\n",
    "    hidden_layers -- array with number of units in hidden layers\n",
    "    epochs -- number of iteration updates through dataset for training (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "    \n",
    "    Returns:\n",
    "    model -- dictionary \n",
    "        PARAMS -- parameters\n",
    "        LOSS -- log of training loss\n",
    "        GRADS -- final \n",
    "        ACC -- array with training and testing accuracies\n",
    "        LR -- learning rate\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (6 lines)\n",
    "    params = model_config(X_train, Y_train, hidden_layers)\n",
    "    params, grads, loss = model_fit(params, X_train, Y_train, epochs=epochs, learning_rate=learning_rate, verbose=verbose)\n",
    "    Y_hat_train = model_predict(params, X_train)\n",
    "    Y_hat_test = model_predict(params, X_test)\n",
    "    train_acc = 100 * np.mean(Y_hat_train == Y_train)\n",
    "    test_acc = 100 * np.mean(Y_hat_test == Y_test)\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "    print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "        \n",
    "    return {\"PARAMS\": params, \"LOSS\": loss, \"GRADS\": grads, \"ACC\": [train_acc, test_acc], \"LR\": learning_rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Evaluate the following cell to train a single hidden layer network with 7 units on the `catsvsnoncats` dataset. Loss should be decreasing and it will take a few minutes to complete the 2500 training epochs. Interrupt the evaluation if the loss after 200 epochs differs from expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 17  31  56]\n",
      "   [ 22  33  59]\n",
      "   [ 25  35  62]\n",
      "   ...\n",
      "   [  1  28  57]\n",
      "   [  1  26  56]\n",
      "   [  1  22  51]]\n",
      "\n",
      "  [[ 25  36  62]\n",
      "   [ 28  38  64]\n",
      "   [ 30  40  67]\n",
      "   ...\n",
      "   [  1  27  56]\n",
      "   [  1  25  55]\n",
      "   [  2  21  51]]\n",
      "\n",
      "  [[ 32  40  67]\n",
      "   [ 34  42  69]\n",
      "   [ 35  42  70]\n",
      "   ...\n",
      "   [  1  25  55]\n",
      "   [  0  24  54]\n",
      "   [  1  21  51]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]\n",
      "\n",
      "\n",
      " [[[196 192 190]\n",
      "   [193 186 182]\n",
      "   [188 179 174]\n",
      "   ...\n",
      "   [ 90 142 200]\n",
      "   [ 90 142 201]\n",
      "   [ 90 142 201]]\n",
      "\n",
      "  [[230 229 229]\n",
      "   [204 199 197]\n",
      "   [193 186 181]\n",
      "   ...\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]]\n",
      "\n",
      "  [[232 225 224]\n",
      "   [235 234 234]\n",
      "   [208 205 202]\n",
      "   ...\n",
      "   [ 91 144 202]\n",
      "   [ 91 144 202]\n",
      "   [ 92 144 202]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 18  17  15]\n",
      "   [ 14  14  13]\n",
      "   [ 29  29  32]\n",
      "   ...\n",
      "   [ 83  81  81]\n",
      "   [ 84  82  83]\n",
      "   [ 82  81  82]]\n",
      "\n",
      "  [[ 22  20  18]\n",
      "   [ 16  15  14]\n",
      "   [ 25  24  24]\n",
      "   ...\n",
      "   [ 82  80  80]\n",
      "   [ 83  81  82]\n",
      "   [ 82  81  81]]\n",
      "\n",
      "  [[ 45  43  39]\n",
      "   [ 61  59  54]\n",
      "   [ 81  78  74]\n",
      "   ...\n",
      "   [ 83  82  81]\n",
      "   [ 84  82  82]\n",
      "   [ 82  80  81]]]\n",
      "\n",
      "\n",
      " [[[ 82  71  68]\n",
      "   [ 89  83  83]\n",
      "   [100  98 104]\n",
      "   ...\n",
      "   [131 132 137]\n",
      "   [126 124 124]\n",
      "   [105  97  95]]\n",
      "\n",
      "  [[ 95  91  97]\n",
      "   [104 104 113]\n",
      "   [110 115 126]\n",
      "   ...\n",
      "   [135 134 135]\n",
      "   [127 122 119]\n",
      "   [111 105 103]]\n",
      "\n",
      "  [[ 94  85  83]\n",
      "   [ 97  89  90]\n",
      "   [110 109 115]\n",
      "   ...\n",
      "   [136 134 131]\n",
      "   [127 120 117]\n",
      "   [116 108 104]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 96 116 131]\n",
      "   [ 97 115 130]\n",
      "   [103 123 139]\n",
      "   ...\n",
      "   [152 155 157]\n",
      "   [146 149 152]\n",
      "   [130 133 134]]\n",
      "\n",
      "  [[ 90 108 123]\n",
      "   [ 92 108 121]\n",
      "   [100 119 134]\n",
      "   ...\n",
      "   [150 152 155]\n",
      "   [144 146 147]\n",
      "   [134 135 134]]\n",
      "\n",
      "  [[ 86 102 116]\n",
      "   [ 87 103 115]\n",
      "   [ 94 114 127]\n",
      "   ...\n",
      "   [154 156 160]\n",
      "   [146 148 152]\n",
      "   [138 141 142]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[143 155 165]\n",
      "   [184 190 198]\n",
      "   [142 149 155]\n",
      "   ...\n",
      "   [ 99  92 102]\n",
      "   [120  98 102]\n",
      "   [100  84  95]]\n",
      "\n",
      "  [[151 149 139]\n",
      "   [173 179 185]\n",
      "   [105 135 141]\n",
      "   ...\n",
      "   [ 91  87  99]\n",
      "   [119  99 104]\n",
      "   [120  95 101]]\n",
      "\n",
      "  [[204 190 185]\n",
      "   [180 185 195]\n",
      "   [117 155 177]\n",
      "   ...\n",
      "   [ 96  88 101]\n",
      "   [125 103 110]\n",
      "   [120 100 110]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 41  80 116]\n",
      "   [ 41  80 116]\n",
      "   [ 41  78 115]\n",
      "   ...\n",
      "   [ 63  75  98]\n",
      "   [ 60  72  98]\n",
      "   [ 60  70  96]]\n",
      "\n",
      "  [[ 71  90 121]\n",
      "   [ 73  91 123]\n",
      "   [ 74  91 124]\n",
      "   ...\n",
      "   [ 79 101 142]\n",
      "   [ 80 100 140]\n",
      "   [ 82 101 139]]\n",
      "\n",
      "  [[ 71  88 122]\n",
      "   [ 73  92 128]\n",
      "   [ 76  95 131]\n",
      "   ...\n",
      "   [ 81 106 150]\n",
      "   [ 85 108 151]\n",
      "   [ 85 107 149]]]\n",
      "\n",
      "\n",
      " [[[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 24  26  25]\n",
      "   ...\n",
      "   [ 24  29  25]\n",
      "   [ 23  25  22]\n",
      "   [ 20  22  21]]\n",
      "\n",
      "  [[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 22  28  23]\n",
      "   [ 20  23  22]\n",
      "   [ 19  21  21]]\n",
      "\n",
      "  [[ 22  24  22]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 23  27  23]\n",
      "   [ 20  23  21]\n",
      "   [ 18  20  19]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  8   5   0]\n",
      "   [  9   6   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  5   4   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   0]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]]\n",
      "\n",
      "\n",
      " [[[  8  28  53]\n",
      "   [ 14  33  58]\n",
      "   [ 19  35  61]\n",
      "   ...\n",
      "   [ 11  16  35]\n",
      "   [ 10  16  35]\n",
      "   [  9  14  32]]\n",
      "\n",
      "  [[ 15  31  57]\n",
      "   [ 15  32  58]\n",
      "   [ 18  34  60]\n",
      "   ...\n",
      "   [ 13  17  35]\n",
      "   [ 13  17  35]\n",
      "   [ 13  16  35]]\n",
      "\n",
      "  [[ 20  35  61]\n",
      "   [ 19  33  59]\n",
      "   [ 20  33  59]\n",
      "   ...\n",
      "   [ 16  17  35]\n",
      "   [ 16  18  35]\n",
      "   [ 15  17  35]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]]\n",
      " After X_trian\n",
      "Loss after 0 epochs: 0.747\n",
      "Loss after 100 epochs: 0.529\n",
      "Loss after 200 epochs: 0.469\n",
      "Loss after 300 epochs: 0.428\n",
      "Loss after 400 epochs: 0.394\n",
      "Loss after 500 epochs: 0.350\n",
      "Loss after 600 epochs: 0.303\n",
      "Loss after 700 epochs: 0.273\n",
      "Loss after 800 epochs: 0.214\n",
      "Loss after 900 epochs: 0.178\n",
      "Loss after 1000 epochs: 0.141\n",
      "Loss after 1100 epochs: 0.106\n",
      "Loss after 1200 epochs: 0.090\n",
      "Loss after 1300 epochs: 0.077\n",
      "Loss after 1400 epochs: 0.067\n",
      "Loss after 1500 epochs: 0.059\n",
      "Loss after 1600 epochs: 0.052\n",
      "Loss after 1700 epochs: 0.046\n",
      "Loss after 1800 epochs: 0.042\n",
      "Loss after 1900 epochs: 0.037\n",
      "Loss after 2000 epochs: 0.034\n",
      "Loss after 2100 epochs: 0.031\n",
      "Loss after 2200 epochs: 0.028\n",
      "Loss after 2300 epochs: 0.026\n",
      "Loss after 2400 epochs: 0.024\n",
      "100.0% training acc.\n",
      "70.0% test acc.\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> Loss after 0 epochs: 0.747\n",
    "> Loss after 100 epochs: 0.529\n",
    "> Loss after 200 epochs: 0.469\n",
    "> ...\n",
    "> Loss after 2300 epochs: 0.026\n",
    "> Loss after 2400 epochs: 0.024\n",
    "> 100.0% training acc.\n",
    "> 70.0% test acc.\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "The performance is similar to what was achieved with the logistic regression model. Note that training accuracy is 100% while testing is around 70%. This is a sign of overfitting.\n",
    "\n",
    "***\n",
    "\n",
    "Let's now train a 5-layer model on the same dataset. Evaluate the following cell. It will take a few minutes to complete the 2500 epoch of training. Interrupt the evaluation if the loss after 200 epochs differs from expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After X_trian\n",
      "Loss after 0 epochs: 0.772\n",
      "Loss after 100 epochs: 0.666\n",
      "Loss after 200 epochs: 0.642\n",
      "Loss after 300 epochs: 0.610\n",
      "Loss after 400 epochs: 0.565\n",
      "Loss after 500 epochs: 0.499\n",
      "Loss after 600 epochs: 0.434\n",
      "Loss after 700 epochs: 0.373\n",
      "Loss after 800 epochs: 0.313\n",
      "Loss after 900 epochs: 0.253\n",
      "Loss after 1000 epochs: 0.219\n",
      "Loss after 1100 epochs: 0.164\n",
      "Loss after 1200 epochs: 0.123\n",
      "Loss after 1300 epochs: 0.100\n",
      "Loss after 1400 epochs: 0.082\n",
      "Loss after 1500 epochs: 0.068\n",
      "Loss after 1600 epochs: 0.059\n",
      "Loss after 1700 epochs: 0.050\n",
      "Loss after 1800 epochs: 0.044\n",
      "Loss after 1900 epochs: 0.039\n",
      "Loss after 2000 epochs: 0.035\n",
      "Loss after 2100 epochs: 0.031\n",
      "Loss after 2200 epochs: 0.028\n",
      "Loss after 2300 epochs: 0.025\n",
      "Loss after 2400 epochs: 0.023\n",
      "100.0% training acc.\n",
      "78.0% test acc.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 258345\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 9, 7], learning_rate=0.009)\n",
    "\n",
    "plt.plot(model[\"LOSS\"])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(model[\"LR\"]))\n",
    "plt.show()\n",
    "\n",
    "params = model[\"PARAMS\"]\n",
    "parameter_count = 0\n",
    "for key in params.keys():\n",
    "    parameter_count = parameter_count + np.prod(params[key].shape)\n",
    "print(\"Number of trainable parameters: {}\".format(parameter_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    ">```\n",
    "> Loss after 0 epochs: 0.772\n",
    "> ...\n",
    "> Loss after 2200 epochs: 0.028\n",
    "> Loss after 2300 epochs: 0.025\n",
    "> Loss after 2400 epochs: 0.023\n",
    "> 100.0% training acc.\n",
    "> 78.0% test acc.\n",
    "> \n",
    "> Number of trainable parameters: 258345\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "This deeper network has a little over 1/4 million parameters for a dataset of $64\\times 64$ colour images. Although we did outperform our previous models, this is far from efficient. Also our model suffers from overfitting. Tuning hyper-parameters such number of hidden layers, number of units, learning rate or number of epochs also greatly affect performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
