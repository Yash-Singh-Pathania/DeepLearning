{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression #\n",
    "\n",
    "Objectives: built an iterative algorithm to learn how to discriminate images of cats using a logistic regression classification model. We will perform a gradient descent and experiment with one important hyperparameter called the learning rate.\n",
    "\n",
    "> **Instructions:** ensure your Python environment is setup with the following packages: \n",
    ">\n",
    "> - `matplotlib` is used to display images and plot graphs\n",
    "> - `PIL` and `skimage` is used to perform some image processing operations\n",
    "> - `h5py` is used to import dataset from H5 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## A. Preparing your Dataset ##\n",
    "\n",
    "A common pre-processing step in deep learning is to normalise the dataset. First, one computes the mean and standard deviation over the entire set. Then, the mean is subtracted from each item. Finally, data is scaled by the standard deviation. \n",
    "\n",
    "For image datasets, RGB pixel values are represented with 3 integers in the range of 0 to 255. A simpler pre-processing strategy that yields similar performance, is to divide each item by 255. \n",
    "\n",
    "Our dataset is stored in a single file called `catsvsnoncats.h5`. It is a collection of images labelled with ground truth information i.e. $y=1$ for the cat class and $y=0$ for the non-cat class. All images are of shape $(W, H, 3)$ where $W=H=64$ i.e. small square RGB images. We will first write a function to load and preprocess the dataset such that each row represents an image whose pixels values are in the range (0,1). Our dataset is split in 2 sets, one for training and one for test.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Reshape and scale both datasets so that images are flattened into a single vector of shape $(3\\times W\\times H)$ where all item features are within the unit range. Note that a numpy array of shape $(N1,N2,N3,N4)$ can be flattened in an array of shape $(N1, N2\\times N3\\times N4)$ using: \n",
    "> ```python \n",
    " A_flattened = A.reshape(A.shape[0], -1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(filepath='datasets/catsvsnoncats.h5'):\n",
    "    \"\"\"\n",
    "    Load and pre-process dataset\n",
    "    \n",
    "    Arguments:\n",
    "    filepath -- string, dataset path\n",
    "\n",
    "    Returns:\n",
    "    (X_train, Y_train), (X_test, Y_test), classes -- training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    h5file = h5py.File(filepath, \"r\")\n",
    "    X_train = np.array(h5file[\"X_train\"][:])\n",
    "    Y_train = np.array(h5file[\"Y_train\"][:])\n",
    "    X_test = np.array(h5file[\"X_test\"][:])\n",
    "    Y_test = np.array(h5file[\"Y_test\"][:])\n",
    "    classes = np.array(h5file[\"Classes\"][:]) \n",
    "    h5file.close()\n",
    "\n",
    "    # Reshape and scale datasets containing N_train and N_test mages such that\n",
    "    # X_train.shape = (n_train, W * H * 3)\n",
    "    # Y_train.shape = (n_train, 1)\n",
    "    # X_test.shape = (n_test, W * H * 3)\n",
    "    # X_test.shape = (n_test, 1)\n",
    "    # Scale X_train and X_test from range {0..255} to (0,1)\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    Y_train = None\n",
    "    Y_test = None\n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "    return (X_train, Y_train), (X_test, Y_test), classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "print(\"classes: 0={} 1={}\".format(classes[0].decode(), classes[1].decode()))\n",
    "print(\"sanity check: ({},{:.1f})\".format(X_train[-1].min(),X_train[-1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `classes: 0=dog 1=cat\n",
    "> sanity check: (0.0,1.0)`\n",
    "\n",
    "***\n",
    "\n",
    "To visualize a random image from the training set, evaluate the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise a random picture from training set\n",
    "import random\n",
    "index = random.randint(0, Y_train.shape[0])\n",
    "print(\"image class: {}\".format(classes[int(Y_train[index, :])].decode()))\n",
    "_ = plt.imshow(X_train[index].reshape(64,64,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many issues in deep learning can be resolved by inspecting tensor shapes, ensuring that matrices or vectors have appropriate dimensions.\n",
    "\n",
    "***\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Explore dataset dimensions.<br/> Find the values for the number of images in the datasets and the number of features per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUT YOUR CODE HERE ### (3 lines)\n",
    "n_train = None\n",
    "n_test = None\n",
    "feature_count = None\n",
    "### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "print(\"training set: {} images\".format(n_train))\n",
    "print(\"test set: {} images\".format(n_test))\n",
    "print(\"features per image: {} pixel values\".format(feature_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `training set: 209 images\n",
    "test set: 50 images\n",
    "features per image: 12288 pixel values`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Logistic Regression using Backprop ##\n",
    "\n",
    "Logistic regression can be understood as a simple neural network:\n",
    "\n",
    "<img src=\"figs/LogisticRegression.png\" style=\"width:800px\">\n",
    "\n",
    "Given a normalised input $x$ (row vector), the logistic regression prediction is given by $\\hat{y} = \\sigma({x\\, w + b})\\notag$, where $w$ (column vector) and $b$ (scalar) denotes the model parameters. The cross-entropy loss is obtained from: \n",
    "$$\\ell(y, \\hat{y}) =  - y  \\log(\\hat{y}) - (1-y)  \\log(1-\\hat{y})\\notag$$\n",
    "\n",
    "The overall loss for $n$ training examples is computed from the sum all individual losses:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n \\ell(y^{(i)}, \\hat{y}^{(i)})\\tag{1}$$\n",
    "\n",
    "To implement the logistic regression using backprop, we will take the following steps:\n",
    "\n",
    "- Implement the sigmoid helper function\n",
    "- Initialise model weights and bias\n",
    "- Evaluate model prediction loss using forward propagation (inference)\n",
    "- Train model by updating parameters from gradients (backprop) \n",
    "- Evaluate model performance on test dataset\n",
    "\n",
    "***\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Implement the sigmoid function using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid vectorial function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- scalar or numpy array of any size\n",
    "\n",
    "    Returns:\n",
    "    s -- sigmoid(x), scalar or numpy array of any size\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    s = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sigmoid([-1, 0, 1]) = {}\".format(sigmoid(np.array([-1, 0,1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `sigmoid([-1, 0, 1]) = [0.27 0.5  0.73]`\n",
    "\n",
    "***\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Initialise the regression weights and bias to zero.<br> Hint: look up `np.zeros()` in numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter initialisation\n",
    "def init_model_parameters(dim):\n",
    "    \"\"\"\n",
    "    Initialise weights and bias to 0\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of vector w (i.e. number of model parameters)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialised weights: column vector of shape (dim, 1)\n",
    "    b -- initialised bias: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    w = None\n",
    "    b = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "w, b = init_model_parameters(dim)\n",
    "print(\"w.T = {}\".format(w.T))\n",
    "print(\"b = {}\".format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `w.T = [[0. 0.]]\n",
    "b = 0`\n",
    "\n",
    "***\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Compute the forward propagation. Return gradients and loss. \n",
    ">\n",
    "> Note $X$ is matrix of $n$ training examples stacked in rows. The regression output can be understood as the activation of the output node of a neural network: $A = \\sigma(Z) = \\sigma(X\\,w + b)$ of shape $n\\times 1$. $\\hat{Y}=\\mathbb 1\\text{{$A > 1/2$}}$ is the logistic regression predictions for the $n$ training examples (column vector).\n",
    ">\n",
    "> From equation (1), the total loss can be expressed as  $\\mathcal{L}=-\\frac{1}{n}(Y^T \\log\\hat{Y}+(1-Y)^T\\log(1-\\hat{Y}))$.\n",
    ">\n",
    "> The two gradients are provided:\n",
    ">\n",
    "> $$ \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{n}X^T(\\hat{Y}-Y)\\notag$$\n",
    "> $$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{Y}_i-Y_i)\\notag$$\n",
    ">\n",
    "> Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_prop(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression loss and its gradient\n",
    "\n",
    "    Arguments:\n",
    "    w -- weight vector, a numpy array of shape (W * H * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- n data samples (n, W * H * 3)\n",
    "    Y -- ground truth vector (0=dog, 1=cat) of size (n, 1)\n",
    "\n",
    "    Return:\n",
    "    loss -- negative log-likelihood loss for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, same shape as w\n",
    "    db -- gradient of the loss with respect to b, same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Forward prop: compute loss from (X, Y)\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    Y_hat = None\n",
    "    loss = None # Compute cross entropy loss (scalar)\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    loss = np.squeeze(loss)\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    # Compute gradients for backprop\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    dw = None\n",
    "    db = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "\n",
    "    return {\"dw\": dw, \"db\": db}, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1., 2.]]).T, 2., np.array([[1., 2.], [3., -3.], [-2., -1.]]), np.array([[1,1,0]]).T\n",
    "\n",
    "grads, loss = forward_prop(w, b, X, Y)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"dw.T = {}\".format(grads[\"dw\"].T))\n",
    "print(\"db = {:.3f}\".format(grads[\"db\"]))\n",
    "print(\"loss = {:.3f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `dw.T = [[-0.811  0.691]]\n",
    "db = -0.204\n",
    "loss = 0.480`\n",
    "\n",
    "***\n",
    "\n",
    "To train the logistic regression (i.e. fit data in training set), we perform a gradient descent, updating our model parameters, $\\theta \\equiv (w, b)$, iteratively. Such optimisation requires to compute the loss over the training set for the current values of our model parameters $\\theta$ and then update these parameters values according to the following rule:  $\\theta=\\theta-\\lambda\\,d\\theta$ where $\\lambda$ is an hyperparameter scalar called the learning rate. This is generally repeated for a number of iterations described by another optimisation hyperparameter referred to as the epoch count or epochs (1 epoch = a complete presentation of training data).\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Complete the function below to train your logistic regression model parameters $(w, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(w, b, X, Y, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Optimise w and b by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weight vector, a numpy array of shape (W * H * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- n data samples (n, W * H * 3)\n",
    "    Y -- ground truth vector (0=dog, 1=cat) of size (n, 1)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with optimised weights and bias\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "        grads, loss = None # Cost and gradient computation\n",
    "        dw = None # derivative from grads\n",
    "        db = None # derivative from grads\n",
    "        w = None # weights update\n",
    "        b = None # bias update\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss)\n",
    "            print(\"Loss after {} iterations: {:.3f}\".format(i, loss))\n",
    "        \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, losses = model_fit(w, b, X, Y, epochs=800, learning_rate=0.008)\n",
    "print(\"w.T = {}\".format(params[\"w\"].T))\n",
    "print(\"b = {:.3f}\".format(params[\"b\"]))\n",
    "print(\"dw.T = {}\".format(grads[\"dw\"].T))\n",
    "print(\"db = {:.3f}\".format(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    ">`Loss after 0 iterations: 0.480\n",
    "Loss after 100 iterations: 0.116\n",
    "Loss after 200 iterations: 0.064\n",
    "Loss after 300 iterations: 0.046\n",
    "Loss after 400 iterations: 0.037\n",
    "Loss after 500 iterations: 0.031\n",
    "Loss after 600 iterations: 0.027\n",
    "Loss after 700 iterations: 0.024\n",
    "w.T = [[1.958 1.429]]\n",
    "b = 2.110\n",
    "dw.T = [[-0.05   0.011]]\n",
    "db = 0.004`\n",
    "\n",
    "***\n",
    "\n",
    "Once the model parameters are optimised, we can use our logistic regression model to predict labels on our dataset `X_test`. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Implement the `model_predict()` function by computing the activation for input $X$ and converting them to predictions (0/1 values). Avoid python iteration and `if` statements prefering numpy vectorial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference\n",
    "def model_predict(w, b, X):\n",
    "    '''\n",
    "    Predict class label using logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of shape (W * H * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- n data samples (n, W * H * 3)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- vector with class predictions for examples in X\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A = None # Compute Activation, shape (n, 1)\n",
    "    Y_hat = None # Convert activations to {0,1} predictions\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(Y_hat.shape == (n, 1))    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X = np.array([[0.1, -0.2]]).T, 0.2, np.array([[1., 2], [3., -1.], [-2, -3]])\n",
    "print(\"predictions.T = {}\".format(model_predict(w, b, X).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `predictions = [[0 1 1]]`\n",
    "\n",
    "## C. Stiching it all together ##\n",
    "\n",
    "We can now fit all the previous steps into a regression model function that will take our traning and testing dataset as input along with two hyper-parameters (learning rate and number of epochs for training) and that will return the optimised model parameters along training and testing logs useful to study the training behaviour of our gradient optimisation. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Implement the following function and evaluate your model on our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "def logistic_regression_model(X_train, Y_train, X_test, Y_test, epochs=2000, learning_rate=0.5):\n",
    "    '''\n",
    "    Build, train and evalaute the logistic regression model\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set a numpy array of shape (n_train, W * H * 3)\n",
    "    Y_train -- training groud truth vector (0=dog, 1=cat) of size (n_train, 1)\n",
    "    X_test -- testing set a numpy array of shape (n_test, W * H * 3)\n",
    "    Y_test -- testing groud truth vector (0=dog, 1=cat) of size (n_test, 1)\n",
    "    epochs -- number of iteration updates through dataset for training (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "    \n",
    "    Returns:\n",
    "    model -- dictionary with model parameters, training logs and testing results\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "    dim = None\n",
    "    w, b = None\n",
    "    params, grads, losses = None\n",
    "    Y_hat_train = None\n",
    "    Y_hat_test = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    print(\"{:.1f}% training acc.\".format(100 * (1 - np.mean(np.abs(Y_hat_train - Y_train)))))\n",
    "    print(\"{:.1f}% test acc.\".format(100 * (1 - np.mean(np.abs(Y_hat_test - Y_test)))))\n",
    "    \n",
    "    model = {\"w\": params[\"w\"], \"b\": params[\"b\"], \"losses\": losses, \"LR\": learning_rate,\n",
    "            \"Y_hat_test\": Y_hat_test, \"Y_hat_train\": Y_hat_train}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "model = logistic_regression_model(X_train, Y_train, X_test, Y_test, epochs=1000, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> `Loss after 0 iterations: 0.693\n",
    "Loss after 100 iterations: 0.585\n",
    "Loss after 200 iterations: 0.467\n",
    "Loss after 300 iterations: 0.376\n",
    "Loss after 400 iterations: 0.331\n",
    "Loss after 500 iterations: 0.303\n",
    "Loss after 600 iterations: 0.280\n",
    "Loss after 700 iterations: 0.260\n",
    "Loss after 800 iterations: 0.243\n",
    "Loss after 900 iterations: 0.228\n",
    "96.7% training acc.\n",
    "72.0% test acc.`\n",
    "\n",
    "***\n",
    "\n",
    "**Note** that the training loss decreases regularly and the training accuracy reached the high nineties after 1000 epochs. This tends to indicate that the model has sufficient capacity to fit the training data. Test error is at 72%, which is a good performance for such a simple classifier.\n",
    "\n",
    "In DL, parameter initialisation and training procedure often rely on random number generators. It is good practive to set the seed of the pseudo-random number generator so that your results are reproducable.\n",
    "\n",
    "You can visualise misclassification errors by evaluating the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of misclassification\n",
    "misclassified_images = np.where(model[\"Y_hat_test\"][:,-1] != Y_test[:,-1])[-1]\n",
    "index = misclassified_images[random.randint(0, misclassified_images.shape[0] - 1)]\n",
    "\n",
    "print(\"image class: {}\".format(classes[int(Y_test[index, :])].decode()))\n",
    "print(\"predicted class: {}\".format(classes[int(model[\"Y_hat_test\"][index, :])].decode()))\n",
    "_ = plt.imshow(X_test[index].reshape(64,64,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to monitor the logs of your traning sessions by plotting the loss as function of the epoch count. Evaluate the following code and observe how the loss decreases. If you increase the number of iterations, you could further increase training accuracy but at the cost of test accuracy which will start decreasing. This phenomenon is known as overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "loss_logs = np.squeeze(model['losses'])\n",
    "plt.plot(loss_logs)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(model['LR']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing hyperparameter values is often very important to achieve optimal performance for your model. In particular the learning rate $\\lambda$ will determine how fast we may converge to the optimal parameter values. If $\\lambda$ is too large, we may overshoot the optimal value resulting in poor perfomance while choosing a very small value can result in a slow algorithm. Experiment with the following code and observe the training behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate other learning rates\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for lr in learning_rates:\n",
    "    models[str(lr)] = logistic_regression_model(X_train, Y_train, X_test, Y_test, epochs=2000, learning_rate=lr)\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"losses\"]), label= str(models[str(lr)][\"LR\"]))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model performs quite well on this small dataset. However our classification problem is simple ; amoung natural images, cats have some distinctive features that can form some strong indicators for our classifier. Let's now see how our model performs on a larger dataset of images of only dogs and cats. Run the following code and observe how the performance drops significantly (this may take a few minutes to complete). Our model is too simple to achieve good performance. A well trained deep convolutional neural network would typically achieve a testing accuracy in the high nineties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with a more challenging dataset: dogs vs. cats\n",
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/dogsvscats.h5')\n",
    "model = logistic_regression_model(X_train, Y_train, X_test, Y_test, \n",
    "                                  epochs=20000, learning_rate=0.008)\n",
    "\n",
    "# Plot the learning curve\n",
    "loss_logs = np.squeeze(model['losses'])\n",
    "plt.plot(loss_logs)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(model['LR']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
